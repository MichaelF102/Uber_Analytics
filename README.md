# ğŸš• Uber Ride Analytics â€” End-to-End Data Engineering Project

An end-to-end **data engineering and analytics pipeline** that transforms raw CSV data into structured insights using **Bash, SQLite, and Streamlit**.

This project intentionally mirrors **real-world data workflows** â€” messy data ingestion, command-line processing, analytics persistence, and interactive visualization â€” without relying on heavy frameworks.

---

## ğŸ“Œ Project Overview

### Pipeline Flow

Raw CSV (Kaggle)
â†“
Bash Data Cleaning
â†“
Shell-Based Analytics
â†“
SQLite Storage
â†“
Streamlit Dashboard

yaml
Copy code

- **Domain:** Ride bookings & mobility analytics  
- **Dataset Source:** Kaggle (Uber Ride Analytics)

---

## ğŸ“¥ Stage 1 â€” Dataset Acquisition

- Dataset downloaded from Kaggle
- Raw CSV contained:
  - Missing values
  - Inconsistent date & time formats
  - Noise and invalid records
- Represents how data is typically received in real-world pipelines

---

## ğŸ§¹ Stage 2 â€” Data Cleaning (Bash)

**Script:** `dataclean.sh`

### Cleaning Operations
- Removed invalid and duplicate rows
- Trimmed leading and trailing whitespace
- Normalized NULL values
- Fixed negative numeric values
- Capped extreme outliers
- Standardized categorical text
- Final sanity cleanup

### Why Bash?
- Fast streaming processing
- Works in constrained environments
- Scales to large CSV files
- Fully reproducible and automatable

---

## ğŸ“Š Stage 3 â€” Exploratory Analytics (Shell)

**Script:** `analytics.sh`  
**Tools:** Bash + AWK

### Analytics Generated
- Booking status distribution
- Vehicle demand patterns
- Pickup and drop location popularity
- Revenue metrics
- Cancellation analysis

This stage demonstrates that **meaningful analytics can be performed without Python or Pandas**.

---

## ğŸ—„ï¸ Stage 4 â€” Persisting Analytics (SQLite)

**Script:** `analytics_to_sql.sh`

### Features
- SQLite database creation
- Normalized analytics tables
- Pre-aggregated metrics
- Fast downstream query access

### Why SQLite?
- Lightweight and portable
- Zero configuration
- Ideal for analytics dashboards
- Production-friendly for read-heavy workloads

---

## ğŸ“ˆ Stage 5 â€” Interactive Dashboard (Streamlit)

**Framework:** Streamlit  
**Visualizations:** Plotly  
**Backend:** SQLite + CSV

### Dashboard Features
- SQL-backed KPIs for fast performance
- CSV-backed deep analytics (time trends and correlations)
- Interactive filters
- Modular and maintainable code structure
- Optional code visibility toggle for data cleaning logic

---

## ğŸ—‚ï¸ Project Structure

.
â”œâ”€â”€ data/
â”‚ â”œâ”€â”€ ncr_ride_bookings_dirty.csv
â”‚ â””â”€â”€ ncr_ride_bookings_clean.csv
â”‚
â”œâ”€â”€ scripts/
â”‚ â”œâ”€â”€ dataclean.sh
â”‚ â”œâ”€â”€ analytics.sh
â”‚ â””â”€â”€ analytics_to_sql.sh
â”‚
â”œâ”€â”€ dashboard/
â”‚ â”œâ”€â”€ app.py
â”‚ â””â”€â”€ assets/
â”‚
â”œâ”€â”€ analytics.db
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md

yaml
Copy code

---

## âš™ï¸ Setup & Usage

### 1ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/yourusername/uber-ride-analytics.git
cd uber-ride-analytics
2ï¸âƒ£ Run Data Cleaning
bash
Copy code
bash scripts/dataclean.sh
3ï¸âƒ£ Run Analytics
bash
Copy code
bash scripts/analytics.sh
4ï¸âƒ£ Store Results in SQLite
bash
Copy code
bash scripts/analytics_to_sql.sh
5ï¸âƒ£ Launch the Dashboard
bash
Copy code
pip install -r requirements.txt
streamlit run dashboard/app.py
ğŸ“¦ Requirements
See requirements.txt

Core dependencies:

Streamlit

Pandas

Plotly

SQLite

Pillow

